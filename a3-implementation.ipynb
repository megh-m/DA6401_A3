{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":397615,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":325961,"modelId":346845}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nwandb.login(key = \"eb9574fa5b11da36782604ea27df8bf1989ddefd\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:21.012721Z","iopub.execute_input":"2025-05-16T20:01:21.013393Z","iopub.status.idle":"2025-05-16T20:01:31.210565Z","shell.execute_reply.started":"2025-05-16T20:01:21.013370Z","shell.execute_reply":"2025-05-16T20:01:31.209158Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegh_m\u001b[0m (\u001b[33mmegh_m-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:31.212034Z","iopub.execute_input":"2025-05-16T20:01:31.212552Z","iopub.status.idle":"2025-05-16T20:01:38.737294Z","shell.execute_reply.started":"2025-05-16T20:01:31.212522Z","shell.execute_reply":"2025-05-16T20:01:38.736573Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\nDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.1.0 rapidfuzz-3.13.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_SILENT\"] = \"false\"\nos.environ[\"WANDB_START_METHOD\"] = \"thread\"\nos.environ[\"WANDB_API_KEY\"] = \"eb9574fa5b11da36782604ea27df8bf1989ddefd\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:38.738416Z","iopub.execute_input":"2025-05-16T20:01:38.738677Z","iopub.status.idle":"2025-05-16T20:01:38.742784Z","shell.execute_reply.started":"2025-05-16T20:01:38.738651Z","shell.execute_reply":"2025-05-16T20:01:38.742246Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport random\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:38.744654Z","iopub.execute_input":"2025-05-16T20:01:38.744877Z","iopub.status.idle":"2025-05-16T20:01:47.099936Z","shell.execute_reply.started":"2025-05-16T20:01:38.744860Z","shell.execute_reply":"2025-05-16T20:01:47.099410Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"class CharEmbed(nn.Module):\n    def __init__(self, input_dim, embed_dim):\n        super(CharEmbed, self).__init__()\n        self.embed = nn.Embedding(input_dim, embed_dim)\n    \n    def forward(self, input_seq):\n        return self.embed(input_seq)\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers=1, \n                 cell_type='GRU', dropout=0.0, bidirectional=False):\n        super(EncoderRNN, self).__init__()\n        self.embed = nn.Embedding(input_dim, embed_dim)\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional #to allow forward and backward time step data processing\n        # Cell type options GRU, LSTM & vanilla RNN\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(embed_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n        else: \n            self.rnn = nn.RNN(embed_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0, bidirectional=bidirectional)\n    \n    def forward(self, input_seq, input_lengths, hidden=None):\n        # Sort sequences by length\n        input_lengths, sort_idx = torch.sort(input_lengths, descending=True)\n        input_seq = input_seq[:, sort_idx]  # (seq_len, batch_size, ...)\n        \n        # Convert to embeddings\n        embedded = self.embed(input_seq)\n        \n        # Pack with enforce_sorted=False\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded, \n            input_lengths.cpu(), \n            enforce_sorted=False\n        )\n        \n        # Forward pass\n        outputs, hidden = self.rnn(packed, hidden)\n        \n        # Unpack padding\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n        \n        # Restore original order\n        _, unsort_idx = torch.sort(sort_idx)\n        outputs = outputs[:, unsort_idx]\n        \n        # Handle LSTM hidden/cell states\n        if isinstance(hidden, tuple):\n            hidden = (\n                hidden[0][:, unsort_idx],  # Hidden state\n                hidden[1][:, unsort_idx]   # Cell state\n            )\n        else:  # For GRU/RNN\n            hidden = hidden[:, unsort_idx]\n        \n        return outputs, hidden\n\nclass DecoderRNN(nn.Module): #Basically similar to the encoder, will have a softmax to predict next char\n    def __init__(self, output_dim, embed_dim, hidden_dim, vocab, n_layers=1, cell_type='GRU', dropout=0.0, go_idx=1, stop_idx=2):\n        super(DecoderRNN, self).__init__()\n        self.embed = nn.Embedding(output_dim, embed_dim)\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.n_layers = n_layers\n        self.cell_type = cell_type\n        self.go_idx = go_idx\n        self.stop_idx = stop_idx\n        self.vocab = vocab\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(embed_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0)\n        else:\n            self.rnn = nn.RNN(embed_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0)\n        \n        self.out = nn.Linear(hidden_dim, output_dim)\n        self.softmax = nn.LogSoftmax(dim=1)\n        \n    def forward(self, input, hidden):\n        # Get embedding of current input character\n        embedded = self.embed(input).unsqueeze(0)\n        \n        # Forward pass through decoder\n        output, hidden = self.rnn(embedded, hidden)\n        \n        # Predict next character probabilities\n        output = self.softmax(self.out(output.squeeze(0)))\n        \n        return output, hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:47.100582Z","iopub.execute_input":"2025-05-16T20:01:47.100925Z","iopub.status.idle":"2025-05-16T20:01:47.112593Z","shell.execute_reply.started":"2025-05-16T20:01:47.100907Z","shell.execute_reply":"2025-05-16T20:01:47.111863Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Seq2Seq(nn.Module): #Flexible enough to use different encoders other than the ones we define\n    def __init__(self, input_dim, output_dim, embed_dim, hidden_dim, num_layers, cell_type, dropout, device,vocab, go_idx=1, stop_idx=2):\n        #super().__init__()\n        super(Seq2Seq, self).__init__()\n        self.device = device\n        self.go_idx = go_idx\n        self.stop_idx = stop_idx\n        self.vocab = vocab\n            # Internal encoder creation\n        self.encoder = EncoderRNN(\n            input_dim=input_dim,\n            embed_dim=embed_dim,\n            hidden_dim=hidden_dim,\n            n_layers=num_layers,\n            cell_type=cell_type,\n            dropout=dropout\n        )\n        \n        # Internal decoder creation\n        self.decoder = DecoderRNN(\n            output_dim=output_dim,\n            embed_dim=embed_dim,\n            hidden_dim=hidden_dim,\n            n_layers=num_layers,\n            cell_type=cell_type,\n            dropout=dropout,\n            vocab = vocab,\n            go_idx = go_idx,\n            stop_idx = stop_idx\n        )\n        self.device = device \n    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        \n        # Last hidden state of the encoder\n        encoder_outputs, hidden = self.encoder(src, src_len)\n        \n        # First input to the decoder is the <go> token\n        input = trg[0,:]\n        \n        for t in range(1, trg_len):\n            output, hidden = self.decoder(input, hidden)\n            outputs[t] = output\n            \n            # To Decide if we're going to use teacher forcing or not as needed\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            # Get the highest predicted token from our predictions\n            top1 = output.argmax(1)\n            \n            # If we use teacher forcing, we have to use actual next token as next input\n            # If not, use predicted token\n            input = trg[t] if teacher_force else top1\n        \n        return outputs\n        \n    def beam_search(self, src, src_len, beam_width=5, max_len=50):\n        \"\"\"Batch-friendly beam search implementation\"\"\"\n        self.eval()\n        batch_size = src.size(1)\n        \n        # Initialize beams with GO token\n        beams = torch.full((batch_size * beam_width, max_len), \n                          self.decoder.stop_idx, \n                          device=self.device)\n        beams[:, 0] = self.decoder.go_idx\n        beam_scores = torch.zeros(batch_size * beam_width, device=self.device)\n        \n        # Encode source sequence\n        encoder_outputs, hidden = self.encoder(src, src_len)\n        \n        # Expand hidden states\n        if isinstance(hidden, tuple):  # LSTM\n            hidden = (\n                hidden[0].repeat(1, beam_width, 1),\n                hidden[1].repeat(1, beam_width, 1)\n            )\n        else:  # GRU/RNN\n            hidden = hidden.repeat(1, beam_width, 1)\n        \n        for step in range(max_len-1):\n            decoder_input = beams[:, step]\n            output, hidden = self.decoder.forward(decoder_input, hidden)\n            \n            log_probs = F.log_softmax(output, dim=1)\n            topk_log_probs, topk_indices = torch.topk(log_probs, beam_width, dim=1)\n            \n            # Reshape scores\n            if step == 0:\n                # First step: (batch, beam) -> (batch, beam*beam)\n                expanded_scores = topk_log_probs.view(batch_size, -1)\n            else:\n                # Subsequent steps: (batch, beam, beam) -> (batch, beam*beam)\n                expanded_scores = beam_scores.view(batch_size, beam_width, 1) + \\\n                                 topk_log_probs.view(batch_size, beam_width, beam_width)\n                expanded_scores = expanded_scores.view(batch_size, -1)\n            \n            # Select top candidates\n            top_scores, top_indices = torch.topk(expanded_scores, beam_width, dim=1)\n            \n            # Calculate beam/token origins\n            beam_indices = top_indices // beam_width\n            token_indices = top_indices % beam_width\n            \n            # Update beams with CORRECT indices\n            beams = beams.view(batch_size, beam_width, -1)\n            beams = torch.cat([\n                beams[torch.arange(batch_size)[:, None], beam_indices],\n                token_indices.unsqueeze(-1)  # Correct index usage\n            ], dim=-1)\n            beams = beams.view(batch_size * beam_width, -1)\n            \n            # Update scores and hidden states\n            beam_scores = top_scores.view(-1)\n            if isinstance(hidden, tuple):\n                hidden = (\n                    hidden[0][:, beam_indices.view(-1), :].contiguous(),\n                    hidden[1][:, beam_indices.view(-1), :].contiguous()\n                )\n            else:\n                hidden = hidden[:, beam_indices.view(-1), :].contiguous()\n            \n            # Early stopping check\n            current_tokens = beams[:, step+1]\n            if (current_tokens == self.decoder.stop_idx).all():\n                break\n    \n        return self._process_beams(beams.view(batch_size, beam_width, -1))\n\n    def _process_beams(self, beams_tensor):\n        \"\"\"\n        Converts beam search output tensor into cleaned token sequences.\n        \n        Args:\n            beams_tensor: Tensor of shape (batch_size, beam_width, max_len)\n            \n        Returns:\n            List[List[List[str]]]: For each batch item, a list of beam sequences.\n        \"\"\"\n        batch_size, beam_width, max_len = beams_tensor.size()\n        processed_beams = []\n        \n        for batch_idx in range(batch_size):\n            batch_sequences = []\n            for beam_idx in range(beam_width):\n                # Extract token indices for this beam\n                indices = beams_tensor[batch_idx, beam_idx].tolist()\n                \n                # Remove <go> (go_idx) at the start if present\n                if indices[0] == self.decoder.go_idx:\n                    indices = indices[1:]  # Remove first element\n                \n                # Truncate at first <stop> (stop_idx)\n                try:\n                    stop_pos = indices.index(self.decoder.stop_idx)\n                    indices = indices[:stop_pos]  # Exclude <stop>\n                except ValueError:\n                    pass  # No <stop> found, use all tokens\n                \n                # Remove padding (assuming pad_idx = 0)\n                cleaned_indices = [idx for idx in indices if idx not in [0, self.decoder.go_idx, self.decoder.stop_idx, 3]]\n                \n                # Convert indices to tokens\n                tokens = [self.decoder.vocab.idx2char[idx] for idx in cleaned_indices]\n                batch_sequences.append(tokens)\n            \n            processed_beams.append(batch_sequences)\n        \n        return processed_beams\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:03:39.479054Z","iopub.execute_input":"2025-05-16T20:03:39.479993Z","iopub.status.idle":"2025-05-16T20:03:39.496076Z","shell.execute_reply.started":"2025-05-16T20:03:39.479966Z","shell.execute_reply":"2025-05-16T20:03:39.495389Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Dataset Loading & Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport tarfile\nimport requests\nimport pandas as pd\nfrom io import BytesIO\nfrom collections import defaultdict\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport wandb\n\n# Dataset Configuration\nDATASET_URL = \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\nDATA_DIR = \"./dakshina_dataset\"\nHI_LEXICON_DIR = os.path.join(DATA_DIR,\"dakshina_dataset_v1.0\", \"hi\", \"lexicons\") #For Hindi (Chosen Language)\n\ndef download_and_extract_dataset(): #Scripted Dataset Download\n    if not os.path.exists(DATA_DIR):\n        print(\"Downloading dataset...\")\n        response = requests.get(DATASET_URL)\n        file = tarfile.open(fileobj=BytesIO(response.content))\n        file.extractall(DATA_DIR)\n        print(\"Dataset extracted successfully\")\n\nclass TransliterationVocabulary: #Build Character Vocab and add go,stop, padding and unknown tokens\n    def __init__(self):\n        self.char2idx = defaultdict(lambda: len(self.char2idx))\n        self.idx2char = {}\n        self.special_tokens = ['<pad>', '<go>', '<stop>', '<unk>']\n        \n        # Initialize special tokens\n        for token in self.special_tokens:\n            self.char2idx[token]\n        \n        self.idx2char = {v: k for k, v in self.char2idx.items()}\n    \n    def add_word(self, word):\n        #print(word) #for debugging\n        for char in word:\n            self.char2idx[char]\n        self.idx2char = {v: k for k, v in self.char2idx.items()}\n\nclass TransliterationDataset(Dataset): #Dataset loader for Hindi\n    def __init__(self, split='train'):\n        self.split = split\n        self.data = self._load_data()\n        self.src_vocab = TransliterationVocabulary()\n        self.trg_vocab = TransliterationVocabulary()\n        \n        # Build vocabularies\n        for src,trg in self.data:\n            self.src_vocab.add_word(src)\n            self.trg_vocab.add_word(trg)\n    \n    def _load_data(self):\n        \"\"\"Load data from TSV files and filter non-string entries\"\"\"\n        file_map = {\n            'train': 'hi.translit.sampled.train.tsv',\n            'dev': 'hi.translit.sampled.dev.tsv',\n            'test': 'hi.translit.sampled.test.tsv'\n        }\n        \n        df = pd.read_csv(\n            os.path.join(HI_LEXICON_DIR, file_map[self.split]),\n            sep='\\t', \n            header=None,\n            names=['devanagari', 'latin', 'count'],\n            dtype={'latin': str, 'devanagari': str, 'count':int}  # Force string type\n        )\n        \n        # Filter out non-string entries and empty strings\n        valid_entries = [\n            (latin, devanagari) \n            for latin, devanagari in zip(df['latin'], df['devanagari'])\n            if (isinstance(latin, str) and \n                isinstance(devanagari, str) and\n                len(latin) > 0 and \n                len(devanagari) > 0)\n        ]\n        \n        return valid_entries\n\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        src, trg = self.data[idx]\n        return (\n            [self.src_vocab.char2idx['<go>']] + \n            [self.src_vocab.char2idx[c] for c in src if c not in ['<go>','<stop>','<pad>','<unk>']] +\n            [self.src_vocab.char2idx['<stop>']],\n            [self.trg_vocab.char2idx['<go>']] + \n            [self.trg_vocab.char2idx[c] for c in trg if c not in ['<go>','<stop>','<pad>','<unk>']] +\n            [self.trg_vocab.char2idx['<stop>']]\n        )\n\ndef collate_fn(batch): #Padding and Masking\n    src_batch, trg_batch = zip(*batch)\n    \n    src_lens = torch.tensor([len(x) for x in src_batch])\n    trg_lens = torch.tensor([len(x) for x in trg_batch])\n    \n    src_pad = pad_sequence(\n        [torch.tensor(x) for x in src_batch],\n        padding_value=0  # <pad> token index\n    )\n    \n    trg_pad = pad_sequence(\n        [torch.tensor(x) for x in trg_batch],\n        padding_value=0  # <pad> token index\n    )\n    \n    return src_pad, trg_pad, src_lens, trg_lens\n\ndef get_dataloaders(batch_size=64):\n    \"\"\"Create train, dev, test dataloaders\"\"\"\n    download_and_extract_dataset()\n    \n    train_dataset = TransliterationDataset('train')\n    dev_dataset = TransliterationDataset('dev')\n    test_dataset = TransliterationDataset('test')\n    \n    return (\n        DataLoader(train_dataset, batch_size=batch_size, \n                  shuffle=True, collate_fn=collate_fn),\n        DataLoader(dev_dataset, batch_size=batch_size, \n                 collate_fn=collate_fn),\n        DataLoader(test_dataset, batch_size=batch_size,\n                 collate_fn=collate_fn),\n        train_dataset.src_vocab,\n        train_dataset.trg_vocab\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:47.146649Z","iopub.execute_input":"2025-05-16T20:01:47.146857Z","iopub.status.idle":"2025-05-16T20:01:49.718143Z","shell.execute_reply.started":"2025-05-16T20:01:47.146834Z","shell.execute_reply":"2025-05-16T20:01:49.717596Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"'''\ndf = pd.read_csv(\n            os.path.join(HI_LEXICON_DIR,'hi.translit.sampled.train.tsv'),\n            sep='\\t',  #specifying seperator\n            header=None,\n            names=['devanagari','latin','syllables']\n        )\nlist(zip(df['latin'], df['devanagari']))'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:49.718916Z","iopub.execute_input":"2025-05-16T20:01:49.719335Z","iopub.status.idle":"2025-05-16T20:01:49.724618Z","shell.execute_reply.started":"2025-05-16T20:01:49.719314Z","shell.execute_reply":"2025-05-16T20:01:49.723902Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"\"\\ndf = pd.read_csv(\\n            os.path.join(HI_LEXICON_DIR,'hi.translit.sampled.train.tsv'),\\n            sep='\\t',  #specifying seperator\\n            header=None,\\n            names=['devanagari','latin','syllables']\\n        )\\nlist(zip(df['latin'], df['devanagari']))\""},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom jiwer import cer, wer\nfrom jiwer import visualize_alignment\n\nclass TransliterationMetrics:\n    @staticmethod\n    def preprocess_sequence(indices, vocab, remove_special=True):\n        \"\"\"Convert index tensor to cleaned character sequence\"\"\"\n        chars = []\n        for idx in indices:\n            char = vocab.idx2char[idx]\n            if remove_special and char in ['<go>', '<stop>', '<pad>','<unk>']:\n                continue\n            chars.append(char)\n        return ''.join(chars)\n\ndef evaluate_cer(model, loader, device, beam_width=5):\n    \"\"\"Calculate Character Error Rate\"\"\"\n    model.eval()\n    total_cer = 0.0\n    total = 0\n    \n    with torch.no_grad():\n        for src, trg, src_lens, trg_lens in loader:\n            src = src.to(device)\n            \n            # Get beam search predictions\n            beam_outputs = model.beam_search(src, src_lens, beam_width)\n            \n            # Process batch\n            for i in range(src.size(1)):\n                # Get target sequence\n                target_indices = trg[1:trg_lens[i]-1, i].cpu().tolist()\n                target_str = TransliterationMetrics.preprocess_sequence(target_indices, model.decoder.vocab)\n                \n                # Get top prediction\n                pred_indices = beam_outputs[i][0]\n                pred_str = TransliterationMetrics.preprocess_sequence(pred_indices, model.decoder.vocab)\n                \n                # Calculate CER\n                if target_str:  # Handle empty targets\n                    total_cer += cer(target_str, pred_str)\n                    total += 1\n                else:\n                    total_cer += 1.0  # Penalize completely wrong predictions\n                    total += 1\n                \n    return total_cer / total if total > 0 else 0\n\ndef evaluate_wer(model, loader, device, beam_width=5):\n    \"\"\"Calculate Word Error Rate (for reference)\"\"\"\n    model.eval()\n    total_wer = 0.0\n    total = 0\n    \n    with torch.no_grad():\n        for src, trg, src_lens, trg_lens in loader:\n            src = src.to(device)\n            \n            beam_outputs = model.beam_search(src, src_lens, beam_width)\n            \n            for i in range(src.size(1)):\n                target_indices = trg[1:trg_lens[i]-1, i].cpu().tolist()\n                target_str = TransliterationMetrics.preprocess_sequence(target_indices, model.decoder.vocab)\n                \n                pred_indices = beam_outputs[i][0]\n                pred_str = TransliterationMetrics.preprocess_sequence(pred_indices, model.decoder.vocab)\n                \n                if target_str:\n                    total_wer += wer(target_str, pred_str)\n                    total += 1\n                else:\n                    total_wer += 1.0\n                    total += 1\n                \n    return total_wer / total if total > 0 else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:49.725475Z","iopub.execute_input":"2025-05-16T20:01:49.725744Z","iopub.status.idle":"2025-05-16T20:01:49.772271Z","shell.execute_reply.started":"2025-05-16T20:01:49.725721Z","shell.execute_reply":"2025-05-16T20:01:49.771711Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Train Sweep","metadata":{}},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_char_err', 'goal': 'minimize'},\n    'parameters': {\n        'embedding_size': {'values': [64, 128, 256]},\n        'hidden_size': {'values': [128, 256, 512]},\n        'num_layers': {'values': [1, 2, 3]},\n        'cell_type': {'values': ['LSTM', 'GRU', 'RNN']},\n        'dropout': {'values': [0.2, 0.3]},\n        'learning_rate': {'values': [0.001, 0.0005, 0.0001]},\n        'batch_size': {'values': [32, 64, 128]},\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:49.774509Z","iopub.execute_input":"2025-05-16T20:01:49.775123Z","iopub.status.idle":"2025-05-16T20:01:49.786378Z","shell.execute_reply.started":"2025-05-16T20:01:49.775101Z","shell.execute_reply":"2025-05-16T20:01:49.785858Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train(config=None):\n    with wandb.init(project=\"DA6401_A3\",settings=wandb.Settings(start_method=\"thread\",_disable_stats=True), config = config) as run:\n        config = run.config\n        \n        # Get dataloaders and vocabularies\n        train_loader, dev_loader, _, src_vocab, trg_vocab = get_dataloaders(\n            batch_size=config.batch_size\n        )\n        go_idx = trg_vocab.char2idx['<go>']\n        stop_idx = trg_vocab.char2idx['stop']\n        # Initialize model\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        model = Seq2Seq(\n            input_dim=len(src_vocab.char2idx),\n            output_dim=len(trg_vocab.char2idx),\n            embed_dim=config.embedding_size,\n            hidden_dim=config.hidden_size,\n            num_layers=config.num_layers,\n            cell_type=config.cell_type,\n            dropout=config.dropout,\n            device=device,\n            go_idx = go_idx,\n            stop_idx = stop_idx,\n            vocab = trg_vocab\n        ).to(device)\n        \n        # Training setup\n        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n        \n        # Training loop\n        for epoch in range(15):  # Fixed epoch count for sweep\n            model.train()\n            total_loss = 0\n            \n            for src, trg, src_lens, trg_lens in train_loader:\n                src = src.to(device)\n                trg = trg.to(device)\n                \n                optimizer.zero_grad()\n                output = model(src, src_lens, trg)\n                \n                # Calculate loss\n                output_dim = output.shape[-1]\n                output = output[1:].view(-1, output_dim)\n                trg = trg[1:].view(-1)\n                \n                loss = criterion(output, trg)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                optimizer.step()\n                \n                total_loss += loss.item()\n            file_path = os.path.join(wandb.run.dir, \"model.pth\")\n            torch.save(model.state_dict(), file_path)\n            wandb.save('model.pth')\n            # Validation\n            #val_cer = evaluate_cer(model, dev_loader, device)\n            #val_wer = evaluate_wer(model, dev_loader, device)\n            val_acc = evaluate(model, dev_loader, device)\n            wandb.log({\n                'epoch': epoch,\n                'train_loss': total_loss/len(train_loader),\n                'val_acc': val_acc\n            })\n\ndef evaluate(model, loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for src, trg, src_lens, trg_lens in loader:\n            src = src.to(device)\n            trg = trg.to(device)\n            \n            outputs = model(src, src_lens, trg, 0)  # No teacher forcing\n            outputs = outputs.argmax(dim=-1)\n            \n            # Calculate accuracy\n            mask = (trg != 0)\n            correct += ((outputs == trg) * mask).sum().item()\n            total += mask.sum().item()\n    \n    return correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:49.786995Z","iopub.execute_input":"2025-05-16T20:01:49.787264Z","iopub.status.idle":"2025-05-16T20:01:49.807008Z","shell.execute_reply.started":"2025-05-16T20:01:49.787231Z","shell.execute_reply":"2025-05-16T20:01:49.806364Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_with_beam(config=None):\n    with wandb.init(project=\"DA6401_A3\",settings=wandb.Settings(start_method=\"thread\",_disable_stats=True), config = config) as run:\n        config = run.config\n        \n        # Get dataloaders and vocabularies\n        train_loader, dev_loader, _, src_vocab, trg_vocab = get_dataloaders(\n            batch_size=config.batch_size\n        )\n        go_idx = trg_vocab.char2idx['<go>']\n        stop_idx = trg_vocab.char2idx['stop']\n        # Initialize model\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        model = Seq2Seq(\n            input_dim=len(src_vocab.char2idx),\n            output_dim=len(trg_vocab.char2idx),\n            embed_dim=config.embedding_size,\n            hidden_dim=config.hidden_size,\n            num_layers=config.num_layers,\n            cell_type=config.cell_type,\n            dropout=config.dropout,\n            device=device,\n            go_idx = go_idx,\n            stop_idx = stop_idx,\n            vocab = trg_vocab\n        ).to(device)\n        \n        # Training setup\n        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n        \n        # Training loop\n        for epoch in range(15):  # Fixed epoch count for sweep\n            model.train()\n            total_loss = 0\n            \n            for src, trg, src_lens, trg_lens in train_loader:\n                src = src.to(device)\n                trg = trg.to(device)\n                \n                optimizer.zero_grad()\n                output = model(src, src_lens, trg)\n                \n                # Calculate loss\n                output_dim = output.shape[-1]\n                output = output[1:].view(-1, output_dim)\n                trg = trg[1:].view(-1)\n                \n                loss = criterion(output, trg)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                optimizer.step()\n                \n                total_loss += loss.item()\n            file_path = os.path.join(wandb.run.dir, \"model.pth\")\n            torch.save(model.state_dict(), file_path)\n            wandb.save('model.pth')\n            # Validation\n            val_cer = evaluate_cer(model, dev_loader, device)\n            val_wer = evaluate_wer(model, dev_loader, device)\n            #val_acc = evaluate(model, dev_loader, device)\n            wandb.log({\n                'epoch': epoch,\n                'train_loss': total_loss/len(train_loader),\n                'val_char_err': val_cer,\n                'val_word_err': val_wer\n            })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:49.807790Z","iopub.execute_input":"2025-05-16T20:01:49.808031Z","iopub.status.idle":"2025-05-16T20:01:49.837513Z","shell.execute_reply.started":"2025-05-16T20:01:49.808011Z","shell.execute_reply":"2025-05-16T20:01:49.836853Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_loader, dev_loader, test_loader, src_vocab, trg_vocab = get_dataloaders()\nprint(f\"Source vocab size: {len(src_vocab.char2idx)}\")\nprint(f\"Target vocab size: {len(trg_vocab.char2idx)}\")\nprint(f\"Training batches: {len(train_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:49.838269Z","iopub.execute_input":"2025-05-16T20:01:49.838508Z","iopub.status.idle":"2025-05-16T20:01:50.438154Z","shell.execute_reply.started":"2025-05-16T20:01:49.838494Z","shell.execute_reply":"2025-05-16T20:01:50.437564Z"}},"outputs":[{"name":"stdout","text":"Source vocab size: 30\nTarget vocab size: 67\nTraining batches: 691\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"#TransliterationDataset('test').data[2][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:50.438833Z","iopub.execute_input":"2025-05-16T20:01:50.439067Z","iopub.status.idle":"2025-05-16T20:01:50.442593Z","shell.execute_reply.started":"2025-05-16T20:01:50.439050Z","shell.execute_reply":"2025-05-16T20:01:50.441842Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"src_vocab.char2idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:50.443518Z","iopub.execute_input":"2025-05-16T20:01:50.444171Z","iopub.status.idle":"2025-05-16T20:01:50.465296Z","shell.execute_reply.started":"2025-05-16T20:01:50.444147Z","shell.execute_reply":"2025-05-16T20:01:50.464740Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"defaultdict(<function __main__.TransliterationVocabulary.__init__.<locals>.<lambda>()>,\n            {'<pad>': 0,\n             '<go>': 1,\n             '<stop>': 2,\n             '<unk>': 3,\n             'a': 4,\n             'n': 5,\n             'k': 6,\n             'g': 7,\n             'i': 8,\n             't': 9,\n             'u': 10,\n             'c': 11,\n             'l': 12,\n             'e': 13,\n             'r': 14,\n             's': 15,\n             'h': 16,\n             'd': 17,\n             'b': 18,\n             'y': 19,\n             'o': 20,\n             'j': 21,\n             'z': 22,\n             'm': 23,\n             'v': 24,\n             'w': 25,\n             'p': 26,\n             'f': 27,\n             'x': 28,\n             'q': 29})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"#src_vocab.idx2char","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:50.465915Z","iopub.execute_input":"2025-05-16T20:01:50.466094Z","iopub.status.idle":"2025-05-16T20:01:50.489429Z","shell.execute_reply.started":"2025-05-16T20:01:50.466081Z","shell.execute_reply":"2025-05-16T20:01:50.488912Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Initialize sweep\nsweep_id = wandb.sweep(\n    sweep=sweep_config,  # Your sweep configuration dictionary\n    project=\"DA6401_A3\",\n    entity=\"megh_m-iit-madras\"\n)\n\n# Run sweep agents\nwandb.agent(sweep_id, function=train_with_beam, count = 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:50.490047Z","iopub.execute_input":"2025-05-16T20:01:50.490281Z","iopub.status.idle":"2025-05-16T20:01:50.510873Z","shell.execute_reply.started":"2025-05-16T20:01:50.490262Z","shell.execute_reply":"2025-05-16T20:01:50.510305Z"},"_kg_hide-output":true},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'# Initialize sweep\\nsweep_id = wandb.sweep(\\n    sweep=sweep_config,  # Your sweep configuration dictionary\\n    project=\"DA6401_A3\",\\n    entity=\"megh_m-iit-madras\"\\n)\\n\\n# Run sweep agents\\nwandb.agent(sweep_id, function=train, count = 10)'"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"def show_alignment_example(model, sample):\n    src, trg = sample\n    pred = model.beam_search(src.unsqueeze(1), [len(src)], beam_width=1)[0][0]\n    \n    target_str = TransliterationMetrics.preprocess_sequence(trg[1:-1], model.decoder.vocab)\n    pred_str = TransliterationMetrics.preprocess_sequence(pred, model.decoder.vocab)\n    \n    print(\"CER:\", cer(target_str, pred_str))\n    print(\"Target:\", target_str)\n    print(\"Predicted:\", pred_str)\n    visualize_alignment(target_str, pred_str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:50.511657Z","iopub.execute_input":"2025-05-16T20:01:50.511974Z","iopub.status.idle":"2025-05-16T20:01:50.526802Z","shell.execute_reply.started":"2025-05-16T20:01:50.511953Z","shell.execute_reply":"2025-05-16T20:01:50.526323Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# For analysis\n'''sample = next(iter(dev_loader))\nshow_alignment_example(model, sample[0][0], sample[1][0])'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:50.527398Z","iopub.execute_input":"2025-05-16T20:01:50.527609Z","iopub.status.idle":"2025-05-16T20:01:50.546107Z","shell.execute_reply.started":"2025-05-16T20:01:50.527595Z","shell.execute_reply":"2025-05-16T20:01:50.545459Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'sample = next(iter(dev_loader))\\nshow_alignment_example(model, sample[0][0], sample[1][0])'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbest_model = AttnSeq2Seq(\n            input_dim=len(src_vocab.char2idx),\n            output_dim=len(trg_vocab.char2idx)+1,\n            embed_dim=256,\n            hidden_dim=256,\n            num_layers=2,\n            cell_type='RNN',\n            dropout=0.2,\n            device=device,\n            go_idx = trg_vocab.char2idx['<go>'],\n            stop_idx = trg_vocab.char2idx['<stop>'],\n            vocab = trg_vocab\n        ).to(device)\nbest_model.load_state_dict(torch.load('/kaggle/input/attn_model_2/pytorch/default/1/attn_model.pth',  weights_only = True))\nbest_model.eval()\nevaluate(best_model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:25:23.785118Z","iopub.execute_input":"2025-05-16T20:25:23.785442Z","iopub.status.idle":"2025-05-16T20:25:24.689603Z","shell.execute_reply.started":"2025-05-16T20:25:23.785420Z","shell.execute_reply":"2025-05-16T20:25:24.688971Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"0.1424018501805054"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"with torch.no_grad():\n        for src, trg, src_lens, trg_lens in test_loader:\n            src = src.to(device)\n            trg = trg.to(device)\n            \n            outputs = best_model(src, src_lens, trg, 0)  # No teacher forcing\n            #outputs = outputs.argmax(dim=-1)\n\noutputs.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:51.207291Z","iopub.status.idle":"2025-05-16T20:01:51.207581Z","shell.execute_reply.started":"2025-05-16T20:01:51.207455Z","shell.execute_reply":"2025-05-16T20:01:51.207470Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = []\nwith torch.no_grad():\n    for src, trg, src_lens, trg_lens in test_loader:\n        src = src.to(device)\n        \n        # Forward pass through encoder\n        encoder_outputs, hidden = best_model.encoder(src, src_lens)\n        \n        # Greedy decoding\n        batch_size = src.size(1)\n        decoder_input = torch.full((1, batch_size), best_model.decoder.go_idx, device=device, dtype=torch.long)\n        \n        predictions = torch.zeros(50, batch_size, device=device, dtype=torch.long)\n\n        for t in range(50):\n            decoder_output, hidden, attn_wts = best_model.decoder(decoder_input.squeeze(0), hidden, encoder_outputs)\n            topi = decoder_output.argmax(1)\n            predictions[t] = topi\n            decoder_input = topi.unsqueeze(0)\n\n        # Process batch\n        for i in range(batch_size):\n            # Get source sequence\n            src_indices = src[:,i].cpu().numpy()\n            src_str = ''.join([src_vocab.idx2char[idx] for idx in src_indices if idx not in [0,1,2,3]])\n            \n            # Get prediction\n            pred_indices = predictions[:,i].cpu().numpy()\n            pred_str = ''.join([trg_vocab.idx2char[idx] for idx in pred_indices if idx not in [0,1,2,3]])\n            \n            results.append({\n                'Source': src_str,\n                'Prediction': pred_str,\n                'Target': ''.join([trg_vocab.idx2char[idx] for idx in trg[:,i].cpu().numpy() if idx not in [0,1,2,3]])\n            })\n\npd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:36:16.725018Z","iopub.execute_input":"2025-05-16T20:36:16.725335Z","iopub.status.idle":"2025-05-16T20:36:20.141547Z","shell.execute_reply.started":"2025-05-16T20:36:16.725314Z","shell.execute_reply":"2025-05-16T20:36:20.140881Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"            Source            Prediction     Target\n0              ank                   अंक        अंक\n1             anka                  अंका        अंक\n2            ankgi                 अंकगी      अंकगण\n3           anaktn                अनक्तन      अंकिं\n4           ankutn                अंकुटन      अंकिं\n...            ...                   ...        ...\n4497       utzbgnc              उट्जबग्ं   ैिजरचगंल\n4498  utvuancayaab             उत्वुंसाय  ैिईंलुीुा\n4499   utvuancayab            उत्वुंसायब  ैिईंलुीुा\n4500        utvirv              उत्विर्व    ैिडरवूड\n4501       utvirvv  उत्विर्ववववववववववववव    ैिडरवूड\n\n[4502 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Prediction</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ank</td>\n      <td>अंक</td>\n      <td>अंक</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>anka</td>\n      <td>अंका</td>\n      <td>अंक</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ankgi</td>\n      <td>अंकगी</td>\n      <td>अंकगण</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>anaktn</td>\n      <td>अनक्तन</td>\n      <td>अंकिं</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ankutn</td>\n      <td>अंकुटन</td>\n      <td>अंकिं</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4497</th>\n      <td>utzbgnc</td>\n      <td>उट्जबग्ं</td>\n      <td>ैिजरचगंल</td>\n    </tr>\n    <tr>\n      <th>4498</th>\n      <td>utvuancayaab</td>\n      <td>उत्वुंसाय</td>\n      <td>ैिईंलुीुा</td>\n    </tr>\n    <tr>\n      <th>4499</th>\n      <td>utvuancayab</td>\n      <td>उत्वुंसायब</td>\n      <td>ैिईंलुीुा</td>\n    </tr>\n    <tr>\n      <th>4500</th>\n      <td>utvirv</td>\n      <td>उत्विर्व</td>\n      <td>ैिडरवूड</td>\n    </tr>\n    <tr>\n      <th>4501</th>\n      <td>utvirvv</td>\n      <td>उत्विर्ववववववववववववव</td>\n      <td>ैिडरवूड</td>\n    </tr>\n  </tbody>\n</table>\n<p>4502 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"src_vocab.char2idx['a']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:51.210011Z","iopub.status.idle":"2025-05-16T20:01:51.210508Z","shell.execute_reply.started":"2025-05-16T20:01:51.210325Z","shell.execute_reply":"2025-05-16T20:01:51.210343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nwith torch.no_grad():\n    for src, trg, src_lens, trg_lens in test_loader:\n        src = src.to(device)\n        trg = trg.to(device)\n        for i in range(1):\n            # Get source sequence\n            src_indices = src[:,i].cpu().numpy()\n            src_str = ''.join([src_vocab.idx2char[idx] for idx in src_indices if idx not in [0, 1, 2, 3]])\n            print(src_str) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:51.212570Z","iopub.status.idle":"2025-05-16T20:01:51.212931Z","shell.execute_reply.started":"2025-05-16T20:01:51.212767Z","shell.execute_reply":"2025-05-16T20:01:51.212785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src_str","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:51.213782Z","iopub.status.idle":"2025-05-16T20:01:51.214062Z","shell.execute_reply.started":"2025-05-16T20:01:51.213930Z","shell.execute_reply":"2025-05-16T20:01:51.213942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"src, trg = TransliterationDataset('test').data[2]\nfor c in src:\n    print(c)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:02:18.129828Z","iopub.execute_input":"2025-05-16T20:02:18.130357Z","iopub.status.idle":"2025-05-16T20:02:18.172778Z","shell.execute_reply.started":"2025-05-16T20:02:18.130333Z","shell.execute_reply":"2025-05-16T20:02:18.172237Z"}},"outputs":[{"name":"stdout","text":"a\nn\nk\ni\nt\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"[src_vocab.char2idx['<go>']] + [src_vocab.char2idx[c] for c in src if c not in ['<go>','<stop>','<pad>','<unk>']] +[src_vocab.char2idx['<stop>']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:02:22.842320Z","iopub.execute_input":"2025-05-16T20:02:22.843026Z","iopub.status.idle":"2025-05-16T20:02:22.848245Z","shell.execute_reply.started":"2025-05-16T20:02:22.843001Z","shell.execute_reply":"2025-05-16T20:02:22.847591Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[1, 4, 5, 6, 8, 9, 2]"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"# With Attention","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n        super().__init__()\n        # Project encoder outputs to decoder's hidden space\n        self.enc_proj = nn.Linear(enc_hidden_dim, dec_hidden_dim)\n        # Project decoder hidden state\n        self.dec_proj = nn.Linear(dec_hidden_dim, dec_hidden_dim)\n        # Energy layer\n        self.energy = nn.Linear(dec_hidden_dim, 1)\n        \n    def forward(self, decoder_hidden, encoder_outputs):\n        # decoder_hidden: (1, batch_size, dec_hidden_dim)\n        # encoder_outputs: (seq_len, batch_size, enc_hidden_dim)\n        \n        # Project both to same dimension\n        dec_proj = self.dec_proj(decoder_hidden.squeeze(0))  # (batch_size, dec_hidden)\n        enc_proj = self.enc_proj(encoder_outputs)  # (seq_len, batch_size, dec_hidden)\n        \n        # Expand and combine\n        dec_proj = dec_proj.unsqueeze(1)  # (batch_size, 1, dec_hidden)\n        enc_proj = enc_proj.permute(1, 0, 2)  # (batch_size, seq_len, dec_hidden)\n        \n        # Calculate scores\n        scores = self.energy(torch.tanh(dec_proj + enc_proj))  # (batch_size, seq_len, 1)\n        attn_weights = F.softmax(scores, dim=1)\n        context = torch.bmm(attn_weights.permute(0,2,1), enc_proj) \n        return context, attn_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:13:55.863322Z","iopub.execute_input":"2025-05-16T20:13:55.864149Z","iopub.status.idle":"2025-05-16T20:13:55.870062Z","shell.execute_reply.started":"2025-05-16T20:13:55.864114Z","shell.execute_reply":"2025-05-16T20:13:55.869310Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class Attention2(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention2, self).__init__()\n        self.encoder_proj = nn.Linear(hidden_size, hidden_size)\n        self.decoder_proj = nn.Linear(hidden_size, hidden_size)\n        self.energy = nn.Linear(hidden_size, 1)\n        \n    def forward(self, decoder_hidden, encoder_outputs):\n        # decoder_hidden: (num_layers, batch_size, hidden_size)\n        # encoder_outputs: (seq_len, batch_size, hidden_size)\n        \n        # Project decoder hidden state\n        decoder_projected = self.decoder_proj(decoder_hidden[-1].unsqueeze(1))  # (batch_size, 1, hidden_size)\n        #decoder_projected = decoder_projected.permute(1,0,2)\n        #print('decoder:',decoder_projected.size())\n        # Project encoder outputs\n        encoder_projected = self.encoder_proj(encoder_outputs.permute(1,0,2))  # (batch_size, seq_len, hidden_size)\n        #print('encoder:',encoder_projected.size())\n        # Calculate attention scores\n        scores = self.energy(torch.tanh(decoder_projected + encoder_projected))  # (batch_size, seq_len, 1)\n        attn_weights = F.softmax(scores, dim=1)  # (batch_size, seq_len, 1)\n        \n        # Calculate context vector\n        context = torch.bmm(attn_weights.permute(0,2,1), encoder_projected)  # (batch_size, 1, hidden_size)\n        \n        return context.squeeze(1), attn_weights.squeeze(2)  # (batch_size, hidden_size), (batch_size, seq_len)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:13:59.716853Z","iopub.execute_input":"2025-05-16T20:13:59.717134Z","iopub.status.idle":"2025-05-16T20:13:59.722967Z","shell.execute_reply.started":"2025-05-16T20:13:59.717116Z","shell.execute_reply":"2025-05-16T20:13:59.722232Z"},"scrolled":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, output_dim, embed_dim, hidden_dim, vocab, n_layers=1, cell_type='GRU', dropout=0.0, go_idx = 1, stop_idx = 2):\n        super(AttnDecoderRNN, self).__init__()\n        self.embed = nn.Embedding(output_dim, embed_dim)\n        self.attention = Attention2(hidden_dim)\n        self.cell_type = cell_type\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.go_idx = go_idx\n        self.stop_idx = stop_idx\n        self.vocab = vocab\n        \n        # RNN Cell Selection\n        if cell_type == 'GRU':\n            self.rnn = nn.GRU(embed_dim + hidden_dim, hidden_dim, \n                             n_layers, dropout=dropout if n_layers > 1 else 0)\n        elif cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embed_dim + hidden_dim, hidden_dim,\n                              n_layers, dropout=dropout if n_layers > 1 else 0)\n        else:\n            self.rnn = nn.RNN(embed_dim + hidden_dim, hidden_dim,\n                             n_layers, dropout=dropout if n_layers > 1 else 0)\n            \n        self.out = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, encoder_outputs):\n        # input: (batch_size)\n        # hidden: (num_layers, batch_size, hidden_dim)\n        # encoder_outputs: (seq_len, batch_size, hidden_dim)\n        \n        embedded = self.dropout(self.embed(input)).unsqueeze(0)  # (1, batch_size, emb_dim)\n        \n        # Calculate attention context\n        context, attn_weights = self.attention(hidden, encoder_outputs)\n        context = context.unsqueeze(0)  # (1, batch_size, hidden_dim)\n        \n        # Combine embedded input and context\n        rnn_input = torch.cat((embedded, context), dim=2)  # (1, batch_size, emb_dim + hidden_dim)\n        \n        # RNN forward pass\n        output, hidden = self.rnn(rnn_input, hidden)\n        \n        # Final prediction\n        output = self.out(torch.cat((output.squeeze(0), context.squeeze(0)), dim=1))\n        output = F.log_softmax(output, dim=1)\n        \n        return output, hidden, attn_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:14:02.925585Z","iopub.execute_input":"2025-05-16T20:14:02.926342Z","iopub.status.idle":"2025-05-16T20:14:02.933953Z","shell.execute_reply.started":"2025-05-16T20:14:02.926301Z","shell.execute_reply":"2025-05-16T20:14:02.933186Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class AttnSeq2Seq(nn.Module):\n    def __init__(self, input_dim, output_dim, embed_dim, hidden_dim, num_layers, cell_type, dropout, device,vocab, go_idx=1, stop_idx=2):\n        super(AttnSeq2Seq, self).__init__()\n        self.go_idx = go_idx\n        self.stop_idx = stop_idx\n        self.vocab = vocab\n        self.encoder = EncoderRNN(\n            input_dim=input_dim,\n            embed_dim=embed_dim,\n            hidden_dim=hidden_dim,\n            n_layers=num_layers,\n            cell_type=cell_type,\n            dropout=dropout\n        )\n        \n        # Internal decoder creation\n        self.decoder = AttnDecoderRNN(\n            output_dim=output_dim,\n            embed_dim=embed_dim,\n            hidden_dim=hidden_dim,\n            n_layers=num_layers,\n            cell_type=cell_type,\n            dropout=dropout,\n            vocab = vocab,\n            go_idx = go_idx,\n            stop_idx = stop_idx\n        )\n        self.device = device \n        \n    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.out.out_features\n        \n        # Encoder forward pass\n        encoder_outputs, hidden = self.encoder(src, src_len)\n        \n        # Decoder initial input\n        dec_input = trg[0,:]  # <sos> token\n        \n        # Output tensor initialization\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n        \n        for t in range(1, trg_len):\n            dec_output, hidden, attn_weights = self.decoder(\n                dec_input, hidden, encoder_outputs\n            )\n            \n            outputs[t] = dec_output\n            attentions[t] = attn_weights\n            \n            # Teacher forcing decision\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = dec_output.argmax(1)\n            dec_input = trg[t] if teacher_force else top1\n            \n        return outputs, attentions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:26:47.846826Z","iopub.execute_input":"2025-05-16T20:26:47.847350Z","iopub.status.idle":"2025-05-16T20:26:47.855558Z","shell.execute_reply.started":"2025-05-16T20:26:47.847327Z","shell.execute_reply":"2025-05-16T20:26:47.854947Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def train_with_attn(config=None):\n    with wandb.init(project=\"DA6401_A3\",settings=wandb.Settings(start_method=\"thread\",_disable_stats=True), config = config) as run:\n        config = run.config\n        \n        # Get dataloaders and vocabularies\n        train_loader, dev_loader, _, src_vocab, trg_vocab = get_dataloaders(\n            batch_size=config.batch_size\n        )\n        go_idx = trg_vocab.char2idx['<go>']\n        stop_idx = trg_vocab.char2idx['stop']\n        # Initialize model\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        model = AttnSeq2Seq(\n            input_dim=len(src_vocab.char2idx),\n            output_dim=len(trg_vocab.char2idx),\n            embed_dim=config.embedding_size,\n            hidden_dim=config.hidden_size,\n            num_layers=config.num_layers,\n            cell_type=config.cell_type,\n            dropout=config.dropout,\n            device=device,\n            go_idx = go_idx,\n            stop_idx = stop_idx,\n            vocab = trg_vocab\n        ).to(device)\n        \n        # Training setup\n        optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n        \n        # Training loop\n        for epoch in range(15):  # Fixed epoch count for sweep\n            model.train()\n            total_loss = 0\n            \n            for src, trg, src_lens, trg_lens in train_loader:\n                src = src.to(device)\n                trg = trg.to(device)\n                \n                optimizer.zero_grad()\n                output, attn_wts = model(src, src_lens, trg)\n                \n                # Calculate loss\n                output_dim = output.shape[-1]\n                output = output[1:].view(-1, output_dim)\n                trg = trg[1:].view(-1)\n                \n                loss = criterion(output, trg)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                optimizer.step()\n                \n                total_loss += loss.item()\n            file_path = os.path.join(wandb.run.dir, \"attn_model.pth\")\n            torch.save(model.state_dict(), file_path)\n            wandb.save('attn_model.pth')\n            # Validation\n            #val_cer = evaluate_cer(model, dev_loader, device)\n            #val_wer = evaluate_wer(model, dev_loader, device)\n            val_acc = evaluate(model, dev_loader, device)\n            wandb.log({\n                'epoch': epoch,\n                'train_loss': total_loss/len(train_loader),\n                'val_acc': val_acc\n            })\n\ndef evaluate(model, loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for src, trg, src_lens, trg_lens in loader:\n            src = src.to(device)\n            trg = trg.to(device)\n            \n            outputs, attn_wts = model(src, src_lens, trg, 0)  # No teacher forcing\n            outputs = outputs.argmax(dim=-1)\n            \n            # Calculate accuracy\n            mask = (trg != 0)\n            correct += ((outputs == trg) * mask).sum().item()\n            total += mask.sum().item()\n    \n    return correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:14:10.310449Z","iopub.execute_input":"2025-05-16T20:14:10.310917Z","iopub.status.idle":"2025-05-16T20:14:10.320639Z","shell.execute_reply.started":"2025-05-16T20:14:10.310895Z","shell.execute_reply":"2025-05-16T20:14:10.320028Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Initialize sweep\nsweep_id = wandb.sweep(\n    sweep=sweep_config,  # Your sweep configuration dictionary\n    project=\"DA6401_A3\",\n    entity=\"megh_m-iit-madras\"\n)\n\n# Run sweep agents\nwandb.agent(sweep_id, function=train_with_attn, count = 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T20:01:51.228557Z","iopub.status.idle":"2025-05-16T20:01:51.228901Z","shell.execute_reply.started":"2025-05-16T20:01:51.228726Z","shell.execute_reply":"2025-05-16T20:01:51.228741Z"}},"outputs":[],"execution_count":null}]}